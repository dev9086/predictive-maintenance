"""
Complete Database Fix - Handles empty tables and all schema issues
Run this ONCE to fix everything
"""
from db_connect import execute_query, fetch_dataframe, bulk_insert
import logging
from datetime import datetime, timedelta
import random

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def check_machines_table():
    """Check if machines table has data"""
    try:
        result = fetch_dataframe("SELECT COUNT(*) as cnt FROM machines")
        count = result.iloc[0]['cnt']
        logger.info(f"‚úì Found {count} machines in database")
        return count
    except Exception as e:
        logger.error(f"‚ùå Error checking machines: {e}")
        return 0

def seed_machines_if_empty():
    """Create sample machines if table is empty"""
    try:
        count = check_machines_table()
        
        if count == 0:
            logger.info("üìù Creating sample machines...")
            
            machines_data = [
                (1, 'CNC-Lathe-001', 'CNC Lathe', 'Factory Floor A', 'Siemens', '2020-01-15'),
                (2, 'Hydraulic-Press-001', 'Hydraulic Press', 'Factory Floor A', 'Bosch Rexroth', '2019-06-20'),
                (3, 'Milling-Machine-001', 'Milling Machine', 'Factory Floor B', 'Fanuc', '2021-03-10'),
                (4, 'Grinder-001', 'Surface Grinder', 'Factory Floor B', 'Okamoto', '2020-08-05'),
                (5, 'Drill-Press-001', 'Drill Press', 'Factory Floor C', 'DMG Mori', '2019-11-30'),
            ]
            
            bulk_insert(
                'machines',
                ['machine_id', 'machine_name', 'machine_type', 'location', 'manufacturer', 'installation_date'],
                machines_data
            )
            
            logger.info(f"‚úÖ Created {len(machines_data)} sample machines")
            return len(machines_data)
        
        return count
        
    except Exception as e:
        logger.error(f"‚ùå Error seeding machines: {e}")
        return 0

def check_current_schema():
    """Check what columns actually exist in model_predictions"""
    query = """
        SELECT column_name, data_type 
        FROM information_schema.columns 
        WHERE table_name = 'model_predictions'
        ORDER BY ordinal_position
    """
    try:
        df = fetch_dataframe(query)
        logger.info("‚úì Current model_predictions columns:")
        for _, row in df.iterrows():
            print(f"  - {row['column_name']} ({row['data_type']})")
        return df['column_name'].tolist()
    except Exception as e:
        logger.error(f"‚ùå Error checking schema: {e}")
        return []

def fix_model_predictions_table():
    """Fix the model_predictions table structure"""
    
    logger.info("\nüîß Fixing model_predictions table structure...")
    
    statements = [
        # Rename columns if they exist
        """
        DO $$
        BEGIN
            IF EXISTS (
                SELECT 1 FROM information_schema.columns 
                WHERE table_name='model_predictions' AND column_name='ts'
            ) THEN
                ALTER TABLE model_predictions RENAME COLUMN ts TO prediction_timestamp;
                RAISE NOTICE 'Renamed ts to prediction_timestamp';
            END IF;
        EXCEPTION WHEN OTHERS THEN
            RAISE NOTICE 'Column ts may already be renamed';
        END $$;
        """,
        
        """
        DO $$
        BEGIN
            IF EXISTS (
                SELECT 1 FROM information_schema.columns 
                WHERE table_name='model_predictions' AND column_name='failure_prob'
            ) THEN
                ALTER TABLE model_predictions RENAME COLUMN failure_prob TO failure_probability;
                RAISE NOTICE 'Renamed failure_prob to failure_probability';
            END IF;
        EXCEPTION WHEN OTHERS THEN
            RAISE NOTICE 'Column failure_prob may already be renamed';
        END $$;
        """,
        
        # Add missing columns
        """
        DO $$
        BEGIN
            IF NOT EXISTS (
                SELECT 1 FROM information_schema.columns 
                WHERE table_name='model_predictions' AND column_name='anomaly_score'
            ) THEN
                ALTER TABLE model_predictions ADD COLUMN anomaly_score REAL DEFAULT 0.0;
                RAISE NOTICE 'Added anomaly_score column';
            END IF;
        EXCEPTION WHEN OTHERS THEN
            RAISE NOTICE 'Column anomaly_score may already exist';
        END $$;
        """,
        
        """
        DO $$
        BEGIN
            IF NOT EXISTS (
                SELECT 1 FROM information_schema.columns 
                WHERE table_name='model_predictions' AND column_name='model_version'
            ) THEN
                ALTER TABLE model_predictions ADD COLUMN model_version VARCHAR(50) DEFAULT 'v1.0';
                RAISE NOTICE 'Added model_version column';
            END IF;
        EXCEPTION WHEN OTHERS THEN
            RAISE NOTICE 'Column model_version may already exist';
        END $$;
        """
    ]
    
    for stmt in statements:
        try:
            execute_query(stmt)
        except Exception as e:
            logger.warning(f"Statement warning (may be OK): {e}")
    
    logger.info("‚úì Schema updates completed")
    return True

def generate_realistic_predictions_manual(machine_count):
    """Generate realistic predictions using Python (avoids SQL random issues)"""
    
    logger.info(f"\nüìä Generating realistic predictions for {machine_count} machines...")
    
    # Clear old predictions
    try:
        execute_query("DELETE FROM model_predictions")
        logger.info("‚úì Cleared old predictions")
    except Exception as e:
        logger.error(f"Error clearing predictions: {e}")
    
    rows = []
    
    for machine_id in range(1, machine_count + 1):
        # Generate 50 historical predictions per machine for trending
        for days_ago in range(50, 0, -1):
            timestamp = datetime.now() - timedelta(days=days_ago)
            
            # Simulate gradual degradation
            base_risk = 0.05 + (days_ago / 50.0) * 0.15  # Increases over time
            
            # Add randomness
            rand = random.random()
            
            if rand < 0.70:  # 70% healthy
                failure_prob = base_risk + random.uniform(0, 0.15)
                predicted_rul = 15 + random.uniform(0, 45)
            elif rand < 0.90:  # 20% medium
                failure_prob = 0.30 + random.uniform(0, 0.25)
                predicted_rul = 5 + random.uniform(0, 15)
            else:  # 10% high risk
                failure_prob = 0.60 + random.uniform(0, 0.30)
                predicted_rul = 1 + random.uniform(0, 7)
            
            # Cap at realistic values
            failure_prob = min(0.95, max(0.01, failure_prob))
            predicted_rul = max(0.5, predicted_rul)
            
            anomaly_score = random.uniform(-0.15, 0.15)
            
            rows.append((
                machine_id,
                timestamp,
                failure_prob,
                predicted_rul,
                anomaly_score,
                'v1.0'
            ))
    
    # Insert in batches
    logger.info(f"Inserting {len(rows)} prediction records...")
    
    try:
        bulk_insert(
            'model_predictions',
            ['machine_id', 'prediction_timestamp', 'failure_probability', 
             'predicted_rul', 'anomaly_score', 'model_version'],
            rows,
            page_size=500
        )
        logger.info(f"‚úÖ Inserted {len(rows)} predictions successfully!")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error inserting predictions: {e}")
        return False

def show_statistics():
    """Show prediction statistics"""
    try:
        stats_query = """
        SELECT 
            COUNT(*) as total_predictions,
            COUNT(DISTINCT machine_id) as total_machines,
            ROUND(AVG(failure_probability)::numeric, 4) as avg_failure_prob,
            ROUND(MIN(failure_probability)::numeric, 4) as min_failure_prob,
            ROUND(MAX(failure_probability)::numeric, 4) as max_failure_prob,
            ROUND(AVG(predicted_rul)::numeric, 2) as avg_rul,
            ROUND(MIN(predicted_rul)::numeric, 2) as min_rul,
            ROUND(MAX(predicted_rul)::numeric, 2) as max_rul
        FROM model_predictions
        """
        
        stats = fetch_dataframe(stats_query)
        
        if not stats.empty:
            s = stats.iloc[0]
            
            logger.info("\nüìà Prediction Statistics:")
            logger.info(f"  Total Predictions: {s['total_predictions']}")
            logger.info(f"  Machines: {s['total_machines']}")
            logger.info(f"  Failure Probability Range: {float(s['min_failure_prob']):.2%} to {float(s['max_failure_prob']):.2%}")
            logger.info(f"  Average Failure Probability: {float(s['avg_failure_prob']):.2%}")
            logger.info(f"  RUL Range: {float(s['min_rul']):.1f} to {float(s['max_rul']):.1f} days")
            logger.info(f"  Average RUL: {float(s['avg_rul']):.1f} days")
            
            # Check for problematic predictions
            high_query = """
            SELECT COUNT(*) as cnt 
            FROM model_predictions 
            WHERE failure_probability > 0.99
            """
            high_df = fetch_dataframe(high_query)
            high_count = high_df.iloc[0]['cnt']
            
            if high_count > 0:
                logger.warning(f"‚ö†Ô∏è  Found {high_count} predictions with >99% failure (may need attention)")
            else:
                logger.info("‚úì No unrealistic 99.99% predictions found!")
            
            return True
    except Exception as e:
        logger.error(f"Error showing statistics: {e}")
        return False

def verify_fix():
    """Verify everything is working"""
    
    logger.info("\n‚úÖ Verifying fix...")
    
    # Check schema
    columns = check_current_schema()
    
    required_columns = ['prediction_timestamp', 'failure_probability', 'predicted_rul', 'anomaly_score']
    missing = [col for col in required_columns if col not in columns]
    
    if missing:
        logger.error(f"‚ùå Still missing columns: {missing}")
        return False
    
    logger.info("‚úì All required columns present")
    
    # Check data
    query = """
    SELECT 
        machine_id,
        prediction_timestamp,
        ROUND(failure_probability::numeric, 4) as failure_probability,
        ROUND(predicted_rul::numeric, 2) as predicted_rul,
        ROUND(anomaly_score::numeric, 4) as anomaly_score
    FROM model_predictions
    ORDER BY prediction_timestamp DESC
    LIMIT 5
    """
    
    try:
        df = fetch_dataframe(query)
        if df.empty:
            logger.error("‚ùå No predictions found")
            return False
            
        logger.info(f"\n‚úì Sample predictions (latest 5):")
        print(df.to_string(index=False))
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Verification failed: {e}")
        return False

def main():
    """Execute complete fix"""
    
    print("="*70)
    print("üîß COMPLETE DATABASE FIX FOR PREDICTIVE MAINTENANCE")
    print("="*70)
    
    print("\nüìã This will:")
    print("  1. Check and create machines if needed")
    print("  2. Fix column names (ts ‚Üí prediction_timestamp, etc.)")
    print("  3. Add missing columns (anomaly_score, model_version)")
    print("  4. Generate realistic prediction data (no more 99.99%)")
    print("  5. Create historical data for charts")
    
    response = input("\nProceed with fix? (yes/no): ").strip().lower()
    
    if response != 'yes':
        print("‚ùå Fix cancelled")
        return
    
    print("\n" + "="*70)
    
    # Step 1: Check/create machines
    print("\n1Ô∏è‚É£ Checking machines table...")
    machine_count = seed_machines_if_empty()
    
    if machine_count == 0:
        print("‚ùå No machines available. Cannot proceed.")
        return
    
    # Step 2: Fix schema
    print("\n2Ô∏è‚É£ Fixing table structure...")
    if not fix_model_predictions_table():
        print("‚ùå Schema fix failed. Aborting.")
        return
    
    # Step 3: Generate realistic data
    print("\n3Ô∏è‚É£ Generating realistic predictions...")
    if not generate_realistic_predictions_manual(machine_count):
        print("‚ùå Data generation failed. Aborting.")
        return
    
    # Step 4: Show statistics
    print("\n4Ô∏è‚É£ Generating statistics...")
    show_statistics()
    
    # Step 5: Verify
    print("\n5Ô∏è‚É£ Verifying fix...")
    if verify_fix():
        print("\n" + "="*70)
        print("‚úÖ FIX COMPLETE!")
        print("="*70)
        print("\nüéâ Your Streamlit dashboard should now work correctly!")
        print("\nüìä Summary:")
        print(f"  - {machine_count} machines in database")
        print(f"  - {machine_count * 50} prediction records created")
        print("  - All columns fixed and present")
        print("  - Realistic failure probabilities (5-90%)")
        print("\nüöÄ Run: streamlit run src/streamlit_app.py")
    else:
        print("\n‚ùå Verification failed. Please check errors above.")

if __name__ == '__main__':
    main()